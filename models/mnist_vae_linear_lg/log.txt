11:57:36 Training device: cuda
11:57:36 BEGIN TRAINING Epoch [1/10]
11:57:42 TRAINING Epoch [1/10], Batch [16/469], Loss: 320.9764413833618
11:57:42 TRAINING Epoch [1/10], Batch [32/469], Loss: 213.4778938293457
11:57:42 TRAINING Epoch [1/10], Batch [48/469], Loss: 209.04144382476807
11:57:42 TRAINING Epoch [1/10], Batch [64/469], Loss: 203.4097146987915
11:57:42 TRAINING Epoch [1/10], Batch [80/469], Loss: 195.75574207305908
11:57:42 TRAINING Epoch [1/10], Batch [96/469], Loss: 192.2623586654663
11:57:42 TRAINING Epoch [1/10], Batch [112/469], Loss: 190.0656328201294
11:57:42 TRAINING Epoch [1/10], Batch [128/469], Loss: 188.3050537109375
11:57:42 TRAINING Epoch [1/10], Batch [144/469], Loss: 184.12185955047607
11:57:43 TRAINING Epoch [1/10], Batch [160/469], Loss: 180.56999111175537
11:57:43 TRAINING Epoch [1/10], Batch [176/469], Loss: 177.14271068572998
11:57:43 TRAINING Epoch [1/10], Batch [192/469], Loss: 175.82673454284668
11:57:43 TRAINING Epoch [1/10], Batch [208/469], Loss: 174.86813068389893
11:57:43 TRAINING Epoch [1/10], Batch [224/469], Loss: 173.03510761260986
11:57:43 TRAINING Epoch [1/10], Batch [240/469], Loss: 171.41008853912354
11:57:43 TRAINING Epoch [1/10], Batch [256/469], Loss: 171.71052646636963
11:57:43 TRAINING Epoch [1/10], Batch [272/469], Loss: 169.61635398864746
11:57:43 TRAINING Epoch [1/10], Batch [288/469], Loss: 167.10199356079102
11:57:43 TRAINING Epoch [1/10], Batch [304/469], Loss: 168.82964038848877
11:57:43 TRAINING Epoch [1/10], Batch [320/469], Loss: 167.854887008667
11:57:43 TRAINING Epoch [1/10], Batch [336/469], Loss: 166.43334865570068
11:57:43 TRAINING Epoch [1/10], Batch [352/469], Loss: 167.03459358215332
11:57:43 TRAINING Epoch [1/10], Batch [368/469], Loss: 166.81939506530762
11:57:43 TRAINING Epoch [1/10], Batch [384/469], Loss: 164.7130355834961
11:57:43 TRAINING Epoch [1/10], Batch [400/469], Loss: 165.2896375656128
11:57:43 TRAINING Epoch [1/10], Batch [416/469], Loss: 164.90834617614746
11:57:43 TRAINING Epoch [1/10], Batch [432/469], Loss: 166.59768962860107
11:57:44 TRAINING Epoch [1/10], Batch [448/469], Loss: 163.64330768585205
11:57:44 TRAINING Epoch [1/10], Batch [464/469], Loss: 161.97930335998535
11:57:44 BEGIN VALIDATION Epoch [1/10]
11:57:44 VALIDATION Epoch [1/10], Loss: 163.2772674560547
11:57:44 BEGIN TRAINING Epoch [2/10]
11:57:44 TRAINING Epoch [2/10], Batch [16/469], Loss: 163.18782234191895
11:57:44 TRAINING Epoch [2/10], Batch [32/469], Loss: 161.99846172332764
11:57:44 TRAINING Epoch [2/10], Batch [48/469], Loss: 161.85356903076172
11:57:44 TRAINING Epoch [2/10], Batch [64/469], Loss: 160.7936725616455
11:57:44 TRAINING Epoch [2/10], Batch [80/469], Loss: 162.1620111465454
11:57:44 TRAINING Epoch [2/10], Batch [96/469], Loss: 160.10814571380615
11:57:44 TRAINING Epoch [2/10], Batch [112/469], Loss: 161.29227924346924
11:57:45 TRAINING Epoch [2/10], Batch [128/469], Loss: 160.71111583709717
11:57:45 TRAINING Epoch [2/10], Batch [144/469], Loss: 160.08138942718506
11:57:45 TRAINING Epoch [2/10], Batch [160/469], Loss: 158.78744792938232
11:57:45 TRAINING Epoch [2/10], Batch [176/469], Loss: 159.20367527008057
11:57:45 TRAINING Epoch [2/10], Batch [192/469], Loss: 159.97612762451172
11:57:45 TRAINING Epoch [2/10], Batch [208/469], Loss: 158.7991542816162
11:57:45 TRAINING Epoch [2/10], Batch [224/469], Loss: 157.7819700241089
11:57:45 TRAINING Epoch [2/10], Batch [240/469], Loss: 157.4813003540039
11:57:45 TRAINING Epoch [2/10], Batch [256/469], Loss: 157.7654104232788
11:57:45 TRAINING Epoch [2/10], Batch [272/469], Loss: 156.46731281280518
11:57:45 TRAINING Epoch [2/10], Batch [288/469], Loss: 159.77520179748535
11:57:45 TRAINING Epoch [2/10], Batch [304/469], Loss: 158.09471130371094
11:57:45 TRAINING Epoch [2/10], Batch [320/469], Loss: 157.47418212890625
11:57:45 TRAINING Epoch [2/10], Batch [336/469], Loss: 159.11851501464844
11:57:45 TRAINING Epoch [2/10], Batch [352/469], Loss: 156.05340194702148
11:57:45 TRAINING Epoch [2/10], Batch [368/469], Loss: 158.70556259155273
11:57:45 TRAINING Epoch [2/10], Batch [384/469], Loss: 156.3048334121704
11:57:46 TRAINING Epoch [2/10], Batch [400/469], Loss: 154.42148876190186
11:57:46 TRAINING Epoch [2/10], Batch [416/469], Loss: 157.4236650466919
11:57:46 TRAINING Epoch [2/10], Batch [432/469], Loss: 155.5896463394165
11:57:46 TRAINING Epoch [2/10], Batch [448/469], Loss: 157.97935581207275
11:57:46 TRAINING Epoch [2/10], Batch [464/469], Loss: 157.22497844696045
11:57:46 BEGIN VALIDATION Epoch [2/10]
11:57:46 VALIDATION Epoch [2/10], Loss: 155.0061303633678
11:57:46 BEGIN TRAINING Epoch [3/10]
11:57:46 TRAINING Epoch [3/10], Batch [16/469], Loss: 155.34800243377686
11:57:46 TRAINING Epoch [3/10], Batch [32/469], Loss: 154.17575550079346
11:57:46 TRAINING Epoch [3/10], Batch [48/469], Loss: 155.26859951019287
11:57:46 TRAINING Epoch [3/10], Batch [64/469], Loss: 153.92515563964844
11:57:46 TRAINING Epoch [3/10], Batch [80/469], Loss: 154.96789836883545
11:57:47 TRAINING Epoch [3/10], Batch [96/469], Loss: 155.57291221618652
11:57:47 TRAINING Epoch [3/10], Batch [112/469], Loss: 155.13845252990723
11:57:47 TRAINING Epoch [3/10], Batch [128/469], Loss: 152.9549207687378
11:57:47 TRAINING Epoch [3/10], Batch [144/469], Loss: 151.733323097229
11:57:47 TRAINING Epoch [3/10], Batch [160/469], Loss: 154.91479301452637
11:57:47 TRAINING Epoch [3/10], Batch [176/469], Loss: 155.40625
11:57:47 TRAINING Epoch [3/10], Batch [192/469], Loss: 153.3298397064209
11:57:47 TRAINING Epoch [3/10], Batch [208/469], Loss: 154.07942581176758
11:57:47 TRAINING Epoch [3/10], Batch [224/469], Loss: 151.39179515838623
11:57:47 TRAINING Epoch [3/10], Batch [240/469], Loss: 152.69821166992188
11:57:47 TRAINING Epoch [3/10], Batch [256/469], Loss: 154.076397895813
11:57:47 TRAINING Epoch [3/10], Batch [272/469], Loss: 153.04437351226807
11:57:47 TRAINING Epoch [3/10], Batch [288/469], Loss: 153.77695274353027
11:57:47 TRAINING Epoch [3/10], Batch [304/469], Loss: 152.64318084716797
11:57:47 TRAINING Epoch [3/10], Batch [320/469], Loss: 156.02367305755615
11:57:47 TRAINING Epoch [3/10], Batch [336/469], Loss: 155.55234146118164
11:57:47 TRAINING Epoch [3/10], Batch [352/469], Loss: 153.26341819763184
11:57:47 TRAINING Epoch [3/10], Batch [368/469], Loss: 152.71791172027588
11:57:48 TRAINING Epoch [3/10], Batch [384/469], Loss: 153.34682846069336
11:57:48 TRAINING Epoch [3/10], Batch [400/469], Loss: 152.47233772277832
11:57:48 TRAINING Epoch [3/10], Batch [416/469], Loss: 151.40607166290283
11:57:48 TRAINING Epoch [3/10], Batch [432/469], Loss: 152.28973293304443
11:57:48 TRAINING Epoch [3/10], Batch [448/469], Loss: 152.90462970733643
11:57:48 TRAINING Epoch [3/10], Batch [464/469], Loss: 152.64922046661377
11:57:48 BEGIN VALIDATION Epoch [3/10]
11:57:48 VALIDATION Epoch [3/10], Loss: 151.58073560497428
11:57:48 BEGIN TRAINING Epoch [4/10]
11:57:48 TRAINING Epoch [4/10], Batch [16/469], Loss: 151.99771690368652
11:57:48 TRAINING Epoch [4/10], Batch [32/469], Loss: 151.1709909439087
11:57:48 TRAINING Epoch [4/10], Batch [48/469], Loss: 152.20199966430664
11:57:48 TRAINING Epoch [4/10], Batch [64/469], Loss: 152.61577701568604
11:57:49 TRAINING Epoch [4/10], Batch [80/469], Loss: 153.86786365509033
11:57:49 TRAINING Epoch [4/10], Batch [96/469], Loss: 151.64294910430908
11:57:49 TRAINING Epoch [4/10], Batch [112/469], Loss: 150.53708744049072
11:57:49 TRAINING Epoch [4/10], Batch [128/469], Loss: 151.03203773498535
11:57:49 TRAINING Epoch [4/10], Batch [144/469], Loss: 149.1627893447876
11:57:49 TRAINING Epoch [4/10], Batch [160/469], Loss: 150.28044605255127
11:57:49 TRAINING Epoch [4/10], Batch [176/469], Loss: 149.41615104675293
11:57:49 TRAINING Epoch [4/10], Batch [192/469], Loss: 149.59952068328857
11:57:49 TRAINING Epoch [4/10], Batch [208/469], Loss: 149.54804611206055
11:57:49 TRAINING Epoch [4/10], Batch [224/469], Loss: 149.93405723571777
11:57:49 TRAINING Epoch [4/10], Batch [240/469], Loss: 148.82812118530273
11:57:49 TRAINING Epoch [4/10], Batch [256/469], Loss: 150.4513807296753
11:57:49 TRAINING Epoch [4/10], Batch [272/469], Loss: 151.97386169433594
11:57:49 TRAINING Epoch [4/10], Batch [288/469], Loss: 150.86165142059326
11:57:49 TRAINING Epoch [4/10], Batch [304/469], Loss: 151.9515142440796
11:57:49 TRAINING Epoch [4/10], Batch [320/469], Loss: 147.59655857086182
11:57:49 TRAINING Epoch [4/10], Batch [336/469], Loss: 151.24835395812988
11:57:50 TRAINING Epoch [4/10], Batch [352/469], Loss: 149.73263359069824
11:57:50 TRAINING Epoch [4/10], Batch [368/469], Loss: 151.0893096923828
11:57:50 TRAINING Epoch [4/10], Batch [384/469], Loss: 149.3726453781128
11:57:50 TRAINING Epoch [4/10], Batch [400/469], Loss: 149.30593395233154
11:57:50 TRAINING Epoch [4/10], Batch [416/469], Loss: 149.4648551940918
11:57:50 TRAINING Epoch [4/10], Batch [432/469], Loss: 150.2911891937256
11:57:50 TRAINING Epoch [4/10], Batch [448/469], Loss: 150.2883758544922
11:57:50 TRAINING Epoch [4/10], Batch [464/469], Loss: 148.20395374298096
11:57:50 BEGIN VALIDATION Epoch [4/10]
11:57:50 VALIDATION Epoch [4/10], Loss: 149.24534684193284
11:57:50 BEGIN TRAINING Epoch [5/10]
11:57:50 TRAINING Epoch [5/10], Batch [16/469], Loss: 148.69293308258057
11:57:51 TRAINING Epoch [5/10], Batch [32/469], Loss: 149.5182285308838
11:57:51 TRAINING Epoch [5/10], Batch [48/469], Loss: 149.2693862915039
11:57:51 TRAINING Epoch [5/10], Batch [64/469], Loss: 148.74947834014893
11:57:51 TRAINING Epoch [5/10], Batch [80/469], Loss: 150.57469272613525
11:57:51 TRAINING Epoch [5/10], Batch [96/469], Loss: 150.0612907409668
11:57:51 TRAINING Epoch [5/10], Batch [112/469], Loss: 149.72998809814453
11:57:51 TRAINING Epoch [5/10], Batch [128/469], Loss: 149.79143238067627
11:57:51 TRAINING Epoch [5/10], Batch [144/469], Loss: 150.04875373840332
11:57:51 TRAINING Epoch [5/10], Batch [160/469], Loss: 149.31471824645996
11:57:51 TRAINING Epoch [5/10], Batch [176/469], Loss: 149.85195064544678
11:57:51 TRAINING Epoch [5/10], Batch [192/469], Loss: 148.33126068115234
11:57:51 TRAINING Epoch [5/10], Batch [208/469], Loss: 148.56376934051514
11:57:51 TRAINING Epoch [5/10], Batch [224/469], Loss: 147.70009231567383
11:57:51 TRAINING Epoch [5/10], Batch [240/469], Loss: 148.24153804779053
11:57:51 TRAINING Epoch [5/10], Batch [256/469], Loss: 148.1801872253418
11:57:51 TRAINING Epoch [5/10], Batch [272/469], Loss: 149.18662548065186
11:57:51 TRAINING Epoch [5/10], Batch [288/469], Loss: 149.9113531112671
11:57:51 TRAINING Epoch [5/10], Batch [304/469], Loss: 148.16647911071777
11:57:52 TRAINING Epoch [5/10], Batch [320/469], Loss: 145.7306776046753
11:57:52 TRAINING Epoch [5/10], Batch [336/469], Loss: 148.51269054412842
11:57:52 TRAINING Epoch [5/10], Batch [352/469], Loss: 146.88693714141846
11:57:52 TRAINING Epoch [5/10], Batch [368/469], Loss: 147.63918209075928
11:57:52 TRAINING Epoch [5/10], Batch [384/469], Loss: 147.59372329711914
11:57:52 TRAINING Epoch [5/10], Batch [400/469], Loss: 147.80025482177734
11:57:52 TRAINING Epoch [5/10], Batch [416/469], Loss: 148.5882225036621
11:57:52 TRAINING Epoch [5/10], Batch [432/469], Loss: 148.33346939086914
11:57:52 TRAINING Epoch [5/10], Batch [448/469], Loss: 146.73653316497803
11:57:52 TRAINING Epoch [5/10], Batch [464/469], Loss: 146.0632095336914
11:57:52 BEGIN VALIDATION Epoch [5/10]
11:57:52 VALIDATION Epoch [5/10], Loss: 148.85179002979135
11:57:52 BEGIN TRAINING Epoch [6/10]
11:57:53 TRAINING Epoch [6/10], Batch [16/469], Loss: 149.20348358154297
11:57:53 TRAINING Epoch [6/10], Batch [32/469], Loss: 147.36850833892822
11:57:53 TRAINING Epoch [6/10], Batch [48/469], Loss: 149.01626873016357
11:57:53 TRAINING Epoch [6/10], Batch [64/469], Loss: 149.3808307647705
11:57:53 TRAINING Epoch [6/10], Batch [80/469], Loss: 147.9477710723877
11:57:53 TRAINING Epoch [6/10], Batch [96/469], Loss: 147.35869789123535
11:57:53 TRAINING Epoch [6/10], Batch [112/469], Loss: 148.50300884246826
11:57:53 TRAINING Epoch [6/10], Batch [128/469], Loss: 148.48745441436768
11:57:53 TRAINING Epoch [6/10], Batch [144/469], Loss: 146.0602560043335
11:57:53 TRAINING Epoch [6/10], Batch [160/469], Loss: 146.67457103729248
11:57:53 TRAINING Epoch [6/10], Batch [176/469], Loss: 148.1196584701538
11:57:53 TRAINING Epoch [6/10], Batch [192/469], Loss: 147.77702617645264
11:57:53 TRAINING Epoch [6/10], Batch [208/469], Loss: 147.1175651550293
11:57:53 TRAINING Epoch [6/10], Batch [224/469], Loss: 147.03717708587646
11:57:53 TRAINING Epoch [6/10], Batch [240/469], Loss: 147.3123664855957
11:57:53 TRAINING Epoch [6/10], Batch [256/469], Loss: 145.66430568695068
11:57:53 TRAINING Epoch [6/10], Batch [272/469], Loss: 146.22106075286865
11:57:54 TRAINING Epoch [6/10], Batch [288/469], Loss: 146.9047737121582
11:57:54 TRAINING Epoch [6/10], Batch [304/469], Loss: 143.89273357391357
11:57:54 TRAINING Epoch [6/10], Batch [320/469], Loss: 147.48833847045898
11:57:54 TRAINING Epoch [6/10], Batch [336/469], Loss: 145.28592205047607
11:57:54 TRAINING Epoch [6/10], Batch [352/469], Loss: 147.38800048828125
11:57:54 TRAINING Epoch [6/10], Batch [368/469], Loss: 146.0175542831421
11:57:54 TRAINING Epoch [6/10], Batch [384/469], Loss: 145.3603057861328
11:57:54 TRAINING Epoch [6/10], Batch [400/469], Loss: 148.34585571289062
11:57:54 TRAINING Epoch [6/10], Batch [416/469], Loss: 145.92255020141602
11:57:54 TRAINING Epoch [6/10], Batch [432/469], Loss: 147.1556520462036
11:57:54 TRAINING Epoch [6/10], Batch [448/469], Loss: 146.4685182571411
11:57:54 TRAINING Epoch [6/10], Batch [464/469], Loss: 146.20194053649902
11:57:54 BEGIN VALIDATION Epoch [6/10]
11:57:55 VALIDATION Epoch [6/10], Loss: 147.22173685967167
11:57:55 BEGIN TRAINING Epoch [7/10]
11:57:55 TRAINING Epoch [7/10], Batch [16/469], Loss: 147.6128568649292
11:57:55 TRAINING Epoch [7/10], Batch [32/469], Loss: 147.6201229095459
11:57:55 TRAINING Epoch [7/10], Batch [48/469], Loss: 145.90579414367676
11:57:55 TRAINING Epoch [7/10], Batch [64/469], Loss: 146.56990432739258
11:57:55 TRAINING Epoch [7/10], Batch [80/469], Loss: 145.41133880615234
11:57:55 TRAINING Epoch [7/10], Batch [96/469], Loss: 148.4625062942505
11:57:55 TRAINING Epoch [7/10], Batch [112/469], Loss: 147.51563930511475
11:57:55 TRAINING Epoch [7/10], Batch [128/469], Loss: 146.1412057876587
11:57:55 TRAINING Epoch [7/10], Batch [144/469], Loss: 145.7233648300171
11:57:55 TRAINING Epoch [7/10], Batch [160/469], Loss: 147.41928482055664
11:57:55 TRAINING Epoch [7/10], Batch [176/469], Loss: 148.58049488067627
11:57:55 TRAINING Epoch [7/10], Batch [192/469], Loss: 144.87896060943604
11:57:55 TRAINING Epoch [7/10], Batch [208/469], Loss: 146.0723114013672
11:57:55 TRAINING Epoch [7/10], Batch [224/469], Loss: 146.66201210021973
11:57:55 TRAINING Epoch [7/10], Batch [240/469], Loss: 146.17548751831055
11:57:56 TRAINING Epoch [7/10], Batch [256/469], Loss: 146.92512130737305
11:57:56 TRAINING Epoch [7/10], Batch [272/469], Loss: 146.56220054626465
11:57:56 TRAINING Epoch [7/10], Batch [288/469], Loss: 147.15656566619873
11:57:56 TRAINING Epoch [7/10], Batch [304/469], Loss: 145.2684507369995
11:57:56 TRAINING Epoch [7/10], Batch [320/469], Loss: 143.4304666519165
11:57:56 TRAINING Epoch [7/10], Batch [336/469], Loss: 145.0833978652954
11:57:56 TRAINING Epoch [7/10], Batch [352/469], Loss: 145.47473526000977
11:57:56 TRAINING Epoch [7/10], Batch [368/469], Loss: 144.70044994354248
11:57:56 TRAINING Epoch [7/10], Batch [384/469], Loss: 144.9956398010254
11:57:56 TRAINING Epoch [7/10], Batch [400/469], Loss: 145.59849071502686
11:57:56 TRAINING Epoch [7/10], Batch [416/469], Loss: 144.92012214660645
11:57:56 TRAINING Epoch [7/10], Batch [432/469], Loss: 146.10230445861816
11:57:56 TRAINING Epoch [7/10], Batch [448/469], Loss: 144.31971073150635
11:57:56 TRAINING Epoch [7/10], Batch [464/469], Loss: 144.4967737197876
11:57:56 BEGIN VALIDATION Epoch [7/10]
11:57:57 VALIDATION Epoch [7/10], Loss: 146.61855557598645
11:57:57 BEGIN TRAINING Epoch [8/10]
11:57:57 TRAINING Epoch [8/10], Batch [16/469], Loss: 144.58681964874268
11:57:57 TRAINING Epoch [8/10], Batch [32/469], Loss: 144.08127784729004
11:57:57 TRAINING Epoch [8/10], Batch [48/469], Loss: 144.68256092071533
11:57:57 TRAINING Epoch [8/10], Batch [64/469], Loss: 145.8834743499756
11:57:57 TRAINING Epoch [8/10], Batch [80/469], Loss: 144.80697917938232
11:57:57 TRAINING Epoch [8/10], Batch [96/469], Loss: 146.17553234100342
11:57:57 TRAINING Epoch [8/10], Batch [112/469], Loss: 146.37737464904785
11:57:57 TRAINING Epoch [8/10], Batch [128/469], Loss: 143.7823257446289
11:57:57 TRAINING Epoch [8/10], Batch [144/469], Loss: 145.34494495391846
11:57:57 TRAINING Epoch [8/10], Batch [160/469], Loss: 142.97089099884033
11:57:57 TRAINING Epoch [8/10], Batch [176/469], Loss: 146.69668197631836
11:57:57 TRAINING Epoch [8/10], Batch [192/469], Loss: 144.88960456848145
11:57:58 TRAINING Epoch [8/10], Batch [208/469], Loss: 145.94865798950195
11:57:58 TRAINING Epoch [8/10], Batch [224/469], Loss: 147.4439811706543
11:57:58 TRAINING Epoch [8/10], Batch [240/469], Loss: 145.4962034225464
11:57:58 TRAINING Epoch [8/10], Batch [256/469], Loss: 144.87049007415771
11:57:58 TRAINING Epoch [8/10], Batch [272/469], Loss: 145.68557357788086
11:57:58 TRAINING Epoch [8/10], Batch [288/469], Loss: 145.54825019836426
11:57:58 TRAINING Epoch [8/10], Batch [304/469], Loss: 144.97118377685547
11:57:58 TRAINING Epoch [8/10], Batch [320/469], Loss: 144.9090223312378
11:57:58 TRAINING Epoch [8/10], Batch [336/469], Loss: 146.5370740890503
11:57:58 TRAINING Epoch [8/10], Batch [352/469], Loss: 144.50092029571533
11:57:58 TRAINING Epoch [8/10], Batch [368/469], Loss: 144.58817195892334
11:57:58 TRAINING Epoch [8/10], Batch [384/469], Loss: 145.84474658966064
11:57:58 TRAINING Epoch [8/10], Batch [400/469], Loss: 144.29992771148682
11:57:58 TRAINING Epoch [8/10], Batch [416/469], Loss: 143.81475830078125
11:57:58 TRAINING Epoch [8/10], Batch [432/469], Loss: 144.077974319458
11:57:58 TRAINING Epoch [8/10], Batch [448/469], Loss: 145.36724472045898
11:57:58 TRAINING Epoch [8/10], Batch [464/469], Loss: 143.42306327819824
11:57:58 BEGIN VALIDATION Epoch [8/10]
11:57:59 VALIDATION Epoch [8/10], Loss: 145.7126275557506
11:57:59 BEGIN TRAINING Epoch [9/10]
11:57:59 TRAINING Epoch [9/10], Batch [16/469], Loss: 144.31329536437988
11:57:59 TRAINING Epoch [9/10], Batch [32/469], Loss: 144.29841136932373
11:57:59 TRAINING Epoch [9/10], Batch [48/469], Loss: 143.74637508392334
11:57:59 TRAINING Epoch [9/10], Batch [64/469], Loss: 144.58265018463135
11:57:59 TRAINING Epoch [9/10], Batch [80/469], Loss: 142.92396640777588
11:57:59 TRAINING Epoch [9/10], Batch [96/469], Loss: 145.03781032562256
11:57:59 TRAINING Epoch [9/10], Batch [112/469], Loss: 145.2777624130249
11:57:59 TRAINING Epoch [9/10], Batch [128/469], Loss: 145.6516933441162
11:57:59 TRAINING Epoch [9/10], Batch [144/469], Loss: 145.3839340209961
11:57:59 TRAINING Epoch [9/10], Batch [160/469], Loss: 143.73248291015625
11:58:00 TRAINING Epoch [9/10], Batch [176/469], Loss: 142.23386478424072
11:58:00 TRAINING Epoch [9/10], Batch [192/469], Loss: 145.4535608291626
11:58:00 TRAINING Epoch [9/10], Batch [208/469], Loss: 143.7929172515869
11:58:00 TRAINING Epoch [9/10], Batch [224/469], Loss: 143.8069896697998
11:58:00 TRAINING Epoch [9/10], Batch [240/469], Loss: 145.70479774475098
11:58:00 TRAINING Epoch [9/10], Batch [256/469], Loss: 143.6885290145874
11:58:00 TRAINING Epoch [9/10], Batch [272/469], Loss: 143.64905548095703
11:58:00 TRAINING Epoch [9/10], Batch [288/469], Loss: 145.07004261016846
11:58:00 TRAINING Epoch [9/10], Batch [304/469], Loss: 144.78633403778076
11:58:00 TRAINING Epoch [9/10], Batch [320/469], Loss: 145.50360679626465
11:58:00 TRAINING Epoch [9/10], Batch [336/469], Loss: 144.86681938171387
11:58:00 TRAINING Epoch [9/10], Batch [352/469], Loss: 145.56890487670898
11:58:00 TRAINING Epoch [9/10], Batch [368/469], Loss: 143.32609462738037
11:58:00 TRAINING Epoch [9/10], Batch [384/469], Loss: 142.33823108673096
11:58:00 TRAINING Epoch [9/10], Batch [400/469], Loss: 142.7745819091797
11:58:00 TRAINING Epoch [9/10], Batch [416/469], Loss: 144.5246877670288
11:58:00 TRAINING Epoch [9/10], Batch [432/469], Loss: 143.6053867340088
11:58:00 TRAINING Epoch [9/10], Batch [448/469], Loss: 144.39705753326416
11:58:01 TRAINING Epoch [9/10], Batch [464/469], Loss: 142.7638702392578
11:58:01 BEGIN VALIDATION Epoch [9/10]
11:58:01 VALIDATION Epoch [9/10], Loss: 144.68477901024156
11:58:01 BEGIN TRAINING Epoch [10/10]
11:58:01 TRAINING Epoch [10/10], Batch [16/469], Loss: 142.08508682250977
11:58:01 TRAINING Epoch [10/10], Batch [32/469], Loss: 144.39825534820557
11:58:01 TRAINING Epoch [10/10], Batch [48/469], Loss: 143.14502334594727
11:58:01 TRAINING Epoch [10/10], Batch [64/469], Loss: 145.50729179382324
11:58:01 TRAINING Epoch [10/10], Batch [80/469], Loss: 143.19214153289795
11:58:01 TRAINING Epoch [10/10], Batch [96/469], Loss: 143.46167659759521
11:58:01 TRAINING Epoch [10/10], Batch [112/469], Loss: 145.3656406402588
11:58:01 TRAINING Epoch [10/10], Batch [128/469], Loss: 141.3454065322876
11:58:02 TRAINING Epoch [10/10], Batch [144/469], Loss: 142.0040159225464
11:58:02 TRAINING Epoch [10/10], Batch [160/469], Loss: 143.99897575378418
11:58:02 TRAINING Epoch [10/10], Batch [176/469], Loss: 142.23718452453613
11:58:02 TRAINING Epoch [10/10], Batch [192/469], Loss: 141.84027004241943
11:58:02 TRAINING Epoch [10/10], Batch [208/469], Loss: 144.08399772644043
11:58:02 TRAINING Epoch [10/10], Batch [224/469], Loss: 146.58554077148438
11:58:02 TRAINING Epoch [10/10], Batch [240/469], Loss: 142.51764011383057
11:58:02 TRAINING Epoch [10/10], Batch [256/469], Loss: 146.0565881729126
11:58:02 TRAINING Epoch [10/10], Batch [272/469], Loss: 143.1237907409668
11:58:02 TRAINING Epoch [10/10], Batch [288/469], Loss: 146.2185640335083
11:58:02 TRAINING Epoch [10/10], Batch [304/469], Loss: 142.3593692779541
11:58:02 TRAINING Epoch [10/10], Batch [320/469], Loss: 142.69667720794678
11:58:02 TRAINING Epoch [10/10], Batch [336/469], Loss: 145.03925037384033
11:58:02 TRAINING Epoch [10/10], Batch [352/469], Loss: 142.21268939971924
11:58:02 TRAINING Epoch [10/10], Batch [368/469], Loss: 143.9750394821167
11:58:02 TRAINING Epoch [10/10], Batch [384/469], Loss: 145.15601444244385
11:58:02 TRAINING Epoch [10/10], Batch [400/469], Loss: 147.33831691741943
11:58:03 TRAINING Epoch [10/10], Batch [416/469], Loss: 141.91329860687256
11:58:03 TRAINING Epoch [10/10], Batch [432/469], Loss: 143.41795349121094
11:58:03 TRAINING Epoch [10/10], Batch [448/469], Loss: 143.53177452087402
11:58:03 TRAINING Epoch [10/10], Batch [464/469], Loss: 144.57455253601074
11:58:03 BEGIN VALIDATION Epoch [10/10]
11:58:03 VALIDATION Epoch [10/10], Loss: 144.87244695349585
11:58:03 Training complete.
