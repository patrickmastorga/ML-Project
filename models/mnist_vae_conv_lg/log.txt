10:34:06 Training device: cuda
10:34:06 BEGIN TRAINING Epoch [1/10]
10:34:07 TRAINING Epoch [1/10], Batch [16/469], Loss: 371.2104091644287
10:34:07 TRAINING Epoch [1/10], Batch [32/469], Loss: 227.03443241119385
10:34:07 TRAINING Epoch [1/10], Batch [48/469], Loss: 215.027494430542
10:34:07 TRAINING Epoch [1/10], Batch [64/469], Loss: 209.92624855041504
10:34:07 TRAINING Epoch [1/10], Batch [80/469], Loss: 206.14460945129395
10:34:07 TRAINING Epoch [1/10], Batch [96/469], Loss: 206.7743101119995
10:34:07 TRAINING Epoch [1/10], Batch [112/469], Loss: 200.72607612609863
10:34:07 TRAINING Epoch [1/10], Batch [128/469], Loss: 196.61435222625732
10:34:07 TRAINING Epoch [1/10], Batch [144/469], Loss: 194.29076290130615
10:34:07 TRAINING Epoch [1/10], Batch [160/469], Loss: 191.9447727203369
10:34:07 TRAINING Epoch [1/10], Batch [176/469], Loss: 189.35771369934082
10:34:07 TRAINING Epoch [1/10], Batch [192/469], Loss: 182.5973777770996
10:34:07 TRAINING Epoch [1/10], Batch [208/469], Loss: 178.48809909820557
10:34:07 TRAINING Epoch [1/10], Batch [224/469], Loss: 176.69067668914795
10:34:07 TRAINING Epoch [1/10], Batch [240/469], Loss: 175.55869007110596
10:34:07 TRAINING Epoch [1/10], Batch [256/469], Loss: 173.8297815322876
10:34:08 TRAINING Epoch [1/10], Batch [272/469], Loss: 170.09059524536133
10:34:08 TRAINING Epoch [1/10], Batch [288/469], Loss: 172.99555110931396
10:34:08 TRAINING Epoch [1/10], Batch [304/469], Loss: 170.38275051116943
10:34:08 TRAINING Epoch [1/10], Batch [320/469], Loss: 169.18875217437744
10:34:08 TRAINING Epoch [1/10], Batch [336/469], Loss: 167.55111503601074
10:34:08 TRAINING Epoch [1/10], Batch [352/469], Loss: 168.65051746368408
10:34:08 TRAINING Epoch [1/10], Batch [368/469], Loss: 165.44415950775146
10:34:08 TRAINING Epoch [1/10], Batch [384/469], Loss: 168.85330772399902
10:34:08 TRAINING Epoch [1/10], Batch [400/469], Loss: 168.60565280914307
10:34:08 TRAINING Epoch [1/10], Batch [416/469], Loss: 166.7580919265747
10:34:08 TRAINING Epoch [1/10], Batch [432/469], Loss: 164.56916904449463
10:34:08 TRAINING Epoch [1/10], Batch [448/469], Loss: 165.74450016021729
10:34:08 TRAINING Epoch [1/10], Batch [464/469], Loss: 163.6063117980957
10:34:08 BEGIN VALIDATION Epoch [1/10]
10:34:09 VALIDATION Epoch [1/10], Loss: 164.98336888566803
10:34:09 BEGIN TRAINING Epoch [2/10]
10:34:09 TRAINING Epoch [2/10], Batch [16/469], Loss: 164.86815547943115
10:34:09 TRAINING Epoch [2/10], Batch [32/469], Loss: 163.83928775787354
10:34:09 TRAINING Epoch [2/10], Batch [48/469], Loss: 165.6367769241333
10:34:09 TRAINING Epoch [2/10], Batch [64/469], Loss: 163.28712272644043
10:34:09 TRAINING Epoch [2/10], Batch [80/469], Loss: 164.4304904937744
10:34:09 TRAINING Epoch [2/10], Batch [96/469], Loss: 161.4660348892212
10:34:09 TRAINING Epoch [2/10], Batch [112/469], Loss: 162.4476089477539
10:34:09 TRAINING Epoch [2/10], Batch [128/469], Loss: 163.16530227661133
10:34:09 TRAINING Epoch [2/10], Batch [144/469], Loss: 163.04698371887207
10:34:10 TRAINING Epoch [2/10], Batch [160/469], Loss: 163.2481803894043
10:34:10 TRAINING Epoch [2/10], Batch [176/469], Loss: 160.64899826049805
10:34:10 TRAINING Epoch [2/10], Batch [192/469], Loss: 161.10403537750244
10:34:10 TRAINING Epoch [2/10], Batch [208/469], Loss: 162.48087310791016
10:34:10 TRAINING Epoch [2/10], Batch [224/469], Loss: 162.51612091064453
10:34:10 TRAINING Epoch [2/10], Batch [240/469], Loss: 161.76581287384033
10:34:10 TRAINING Epoch [2/10], Batch [256/469], Loss: 162.64432621002197
10:34:10 TRAINING Epoch [2/10], Batch [272/469], Loss: 161.43458938598633
10:34:10 TRAINING Epoch [2/10], Batch [288/469], Loss: 161.89549922943115
10:34:10 TRAINING Epoch [2/10], Batch [304/469], Loss: 161.7626609802246
10:34:10 TRAINING Epoch [2/10], Batch [320/469], Loss: 159.60865211486816
10:34:10 TRAINING Epoch [2/10], Batch [336/469], Loss: 161.4844102859497
10:34:10 TRAINING Epoch [2/10], Batch [352/469], Loss: 158.37068939208984
10:34:10 TRAINING Epoch [2/10], Batch [368/469], Loss: 159.7261848449707
10:34:10 TRAINING Epoch [2/10], Batch [384/469], Loss: 159.57211112976074
10:34:10 TRAINING Epoch [2/10], Batch [400/469], Loss: 160.286714553833
10:34:11 TRAINING Epoch [2/10], Batch [416/469], Loss: 157.7884693145752
10:34:11 TRAINING Epoch [2/10], Batch [432/469], Loss: 158.56230449676514
10:34:11 TRAINING Epoch [2/10], Batch [448/469], Loss: 159.0167055130005
10:34:11 TRAINING Epoch [2/10], Batch [464/469], Loss: 159.61973762512207
10:34:11 BEGIN VALIDATION Epoch [2/10]
10:34:11 VALIDATION Epoch [2/10], Loss: 158.78497333768047
10:34:11 BEGIN TRAINING Epoch [3/10]
10:34:11 TRAINING Epoch [3/10], Batch [16/469], Loss: 159.569317817688
10:34:11 TRAINING Epoch [3/10], Batch [32/469], Loss: 157.30248260498047
10:34:11 TRAINING Epoch [3/10], Batch [48/469], Loss: 157.7089786529541
10:34:11 TRAINING Epoch [3/10], Batch [64/469], Loss: 158.4215955734253
10:34:12 TRAINING Epoch [3/10], Batch [80/469], Loss: 157.1932954788208
10:34:12 TRAINING Epoch [3/10], Batch [96/469], Loss: 159.00650310516357
10:34:12 TRAINING Epoch [3/10], Batch [112/469], Loss: 158.33781242370605
10:34:12 TRAINING Epoch [3/10], Batch [128/469], Loss: 158.5991086959839
10:34:12 TRAINING Epoch [3/10], Batch [144/469], Loss: 157.20647811889648
10:34:12 TRAINING Epoch [3/10], Batch [160/469], Loss: 157.78045272827148
10:34:12 TRAINING Epoch [3/10], Batch [176/469], Loss: 157.54102993011475
10:34:12 TRAINING Epoch [3/10], Batch [192/469], Loss: 156.9493169784546
10:34:12 TRAINING Epoch [3/10], Batch [208/469], Loss: 155.93221759796143
10:34:12 TRAINING Epoch [3/10], Batch [224/469], Loss: 156.93888187408447
10:34:12 TRAINING Epoch [3/10], Batch [240/469], Loss: 156.95423126220703
10:34:12 TRAINING Epoch [3/10], Batch [256/469], Loss: 156.74257564544678
10:34:12 TRAINING Epoch [3/10], Batch [272/469], Loss: 155.46756839752197
10:34:12 TRAINING Epoch [3/10], Batch [288/469], Loss: 159.35184955596924
10:34:12 TRAINING Epoch [3/10], Batch [304/469], Loss: 156.3707046508789
10:34:13 TRAINING Epoch [3/10], Batch [320/469], Loss: 156.42548370361328
10:34:13 TRAINING Epoch [3/10], Batch [336/469], Loss: 156.238374710083
10:34:13 TRAINING Epoch [3/10], Batch [352/469], Loss: 155.9519443511963
10:34:13 TRAINING Epoch [3/10], Batch [368/469], Loss: 156.0137300491333
10:34:13 TRAINING Epoch [3/10], Batch [384/469], Loss: 154.6178493499756
10:34:13 TRAINING Epoch [3/10], Batch [400/469], Loss: 155.8699722290039
10:34:13 TRAINING Epoch [3/10], Batch [416/469], Loss: 154.71376132965088
10:34:13 TRAINING Epoch [3/10], Batch [432/469], Loss: 156.4504861831665
10:34:13 TRAINING Epoch [3/10], Batch [448/469], Loss: 155.2909450531006
10:34:13 TRAINING Epoch [3/10], Batch [464/469], Loss: 156.94636154174805
10:34:13 BEGIN VALIDATION Epoch [3/10]
10:34:14 VALIDATION Epoch [3/10], Loss: 155.48916452142257
10:34:14 BEGIN TRAINING Epoch [4/10]
10:34:14 TRAINING Epoch [4/10], Batch [16/469], Loss: 154.16937160491943
10:34:14 TRAINING Epoch [4/10], Batch [32/469], Loss: 155.9609251022339
10:34:14 TRAINING Epoch [4/10], Batch [48/469], Loss: 154.2557258605957
10:34:14 TRAINING Epoch [4/10], Batch [64/469], Loss: 155.8494758605957
10:34:14 TRAINING Epoch [4/10], Batch [80/469], Loss: 154.75065517425537
10:34:14 TRAINING Epoch [4/10], Batch [96/469], Loss: 155.399658203125
10:34:14 TRAINING Epoch [4/10], Batch [112/469], Loss: 154.31259059906006
10:34:14 TRAINING Epoch [4/10], Batch [128/469], Loss: 157.24657154083252
10:34:14 TRAINING Epoch [4/10], Batch [144/469], Loss: 155.87908172607422
10:34:14 TRAINING Epoch [4/10], Batch [160/469], Loss: 156.7546100616455
10:34:14 TRAINING Epoch [4/10], Batch [176/469], Loss: 155.27032279968262
10:34:14 TRAINING Epoch [4/10], Batch [192/469], Loss: 154.84509468078613
10:34:14 TRAINING Epoch [4/10], Batch [208/469], Loss: 153.96971130371094
10:34:15 TRAINING Epoch [4/10], Batch [224/469], Loss: 153.34710693359375
10:34:15 TRAINING Epoch [4/10], Batch [240/469], Loss: 153.07850551605225
10:34:15 TRAINING Epoch [4/10], Batch [256/469], Loss: 154.77118968963623
10:34:15 TRAINING Epoch [4/10], Batch [272/469], Loss: 154.20174503326416
10:34:15 TRAINING Epoch [4/10], Batch [288/469], Loss: 153.39433097839355
10:34:15 TRAINING Epoch [4/10], Batch [304/469], Loss: 154.50050830841064
10:34:15 TRAINING Epoch [4/10], Batch [320/469], Loss: 153.26717281341553
10:34:15 TRAINING Epoch [4/10], Batch [336/469], Loss: 153.4729299545288
10:34:15 TRAINING Epoch [4/10], Batch [352/469], Loss: 156.17428398132324
10:34:15 TRAINING Epoch [4/10], Batch [368/469], Loss: 155.00224208831787
10:34:15 TRAINING Epoch [4/10], Batch [384/469], Loss: 153.44442081451416
10:34:15 TRAINING Epoch [4/10], Batch [400/469], Loss: 153.37447452545166
10:34:15 TRAINING Epoch [4/10], Batch [416/469], Loss: 152.8817949295044
10:34:15 TRAINING Epoch [4/10], Batch [432/469], Loss: 151.82478427886963
10:34:15 TRAINING Epoch [4/10], Batch [448/469], Loss: 154.23582649230957
10:34:16 TRAINING Epoch [4/10], Batch [464/469], Loss: 153.84592533111572
10:34:16 BEGIN VALIDATION Epoch [4/10]
10:34:16 VALIDATION Epoch [4/10], Loss: 153.28814909729778
10:34:16 BEGIN TRAINING Epoch [5/10]
10:34:16 TRAINING Epoch [5/10], Batch [16/469], Loss: 151.5046272277832
10:34:16 TRAINING Epoch [5/10], Batch [32/469], Loss: 152.59855937957764
10:34:16 TRAINING Epoch [5/10], Batch [48/469], Loss: 150.52785205841064
10:34:16 TRAINING Epoch [5/10], Batch [64/469], Loss: 153.8975305557251
10:34:16 TRAINING Epoch [5/10], Batch [80/469], Loss: 154.2765998840332
10:34:16 TRAINING Epoch [5/10], Batch [96/469], Loss: 151.74169826507568
10:34:16 TRAINING Epoch [5/10], Batch [112/469], Loss: 153.37338638305664
10:34:17 TRAINING Epoch [5/10], Batch [128/469], Loss: 152.40392208099365
10:34:17 TRAINING Epoch [5/10], Batch [144/469], Loss: 152.09228038787842
10:34:17 TRAINING Epoch [5/10], Batch [160/469], Loss: 152.1222801208496
10:34:17 TRAINING Epoch [5/10], Batch [176/469], Loss: 153.61943531036377
10:34:17 TRAINING Epoch [5/10], Batch [192/469], Loss: 149.37260818481445
10:34:17 TRAINING Epoch [5/10], Batch [208/469], Loss: 152.32548999786377
10:34:17 TRAINING Epoch [5/10], Batch [224/469], Loss: 152.22142887115479
10:34:17 TRAINING Epoch [5/10], Batch [240/469], Loss: 150.68889999389648
10:34:17 TRAINING Epoch [5/10], Batch [256/469], Loss: 155.326021194458
10:34:17 TRAINING Epoch [5/10], Batch [272/469], Loss: 152.06961727142334
10:34:17 TRAINING Epoch [5/10], Batch [288/469], Loss: 152.55625915527344
10:34:17 TRAINING Epoch [5/10], Batch [304/469], Loss: 153.41543579101562
10:34:17 TRAINING Epoch [5/10], Batch [320/469], Loss: 153.74804592132568
10:34:17 TRAINING Epoch [5/10], Batch [336/469], Loss: 153.57383823394775
10:34:17 TRAINING Epoch [5/10], Batch [352/469], Loss: 152.73635864257812
10:34:18 TRAINING Epoch [5/10], Batch [368/469], Loss: 153.38787651062012
10:34:18 TRAINING Epoch [5/10], Batch [384/469], Loss: 153.81935596466064
10:34:18 TRAINING Epoch [5/10], Batch [400/469], Loss: 151.51757049560547
10:34:18 TRAINING Epoch [5/10], Batch [416/469], Loss: 152.5636625289917
10:34:18 TRAINING Epoch [5/10], Batch [432/469], Loss: 152.62762928009033
10:34:18 TRAINING Epoch [5/10], Batch [448/469], Loss: 152.37232875823975
10:34:18 TRAINING Epoch [5/10], Batch [464/469], Loss: 153.31084823608398
10:34:18 BEGIN VALIDATION Epoch [5/10]
10:34:18 VALIDATION Epoch [5/10], Loss: 152.2462467241891
10:34:18 BEGIN TRAINING Epoch [6/10]
10:34:19 TRAINING Epoch [6/10], Batch [16/469], Loss: 152.01203632354736
10:34:19 TRAINING Epoch [6/10], Batch [32/469], Loss: 153.45817375183105
10:34:19 TRAINING Epoch [6/10], Batch [48/469], Loss: 152.31856060028076
10:34:19 TRAINING Epoch [6/10], Batch [64/469], Loss: 151.77990436553955
10:34:19 TRAINING Epoch [6/10], Batch [80/469], Loss: 152.3124542236328
10:34:19 TRAINING Epoch [6/10], Batch [96/469], Loss: 152.0982666015625
10:34:19 TRAINING Epoch [6/10], Batch [112/469], Loss: 152.65902519226074
10:34:19 TRAINING Epoch [6/10], Batch [128/469], Loss: 152.37306308746338
10:34:19 TRAINING Epoch [6/10], Batch [144/469], Loss: 151.64819145202637
10:34:19 TRAINING Epoch [6/10], Batch [160/469], Loss: 152.8755111694336
10:34:19 TRAINING Epoch [6/10], Batch [176/469], Loss: 150.23817920684814
10:34:19 TRAINING Epoch [6/10], Batch [192/469], Loss: 152.1811990737915
10:34:19 TRAINING Epoch [6/10], Batch [208/469], Loss: 149.98014068603516
10:34:19 TRAINING Epoch [6/10], Batch [224/469], Loss: 152.06834506988525
10:34:19 TRAINING Epoch [6/10], Batch [240/469], Loss: 151.40586471557617
10:34:19 TRAINING Epoch [6/10], Batch [256/469], Loss: 152.12726020812988
10:34:20 TRAINING Epoch [6/10], Batch [272/469], Loss: 150.67120742797852
10:34:20 TRAINING Epoch [6/10], Batch [288/469], Loss: 149.968731880188
10:34:20 TRAINING Epoch [6/10], Batch [304/469], Loss: 152.85364627838135
10:34:20 TRAINING Epoch [6/10], Batch [320/469], Loss: 149.81124782562256
10:34:20 TRAINING Epoch [6/10], Batch [336/469], Loss: 151.53216075897217
10:34:20 TRAINING Epoch [6/10], Batch [352/469], Loss: 150.22221755981445
10:34:20 TRAINING Epoch [6/10], Batch [368/469], Loss: 152.45959854125977
10:34:20 TRAINING Epoch [6/10], Batch [384/469], Loss: 150.71806049346924
10:34:20 TRAINING Epoch [6/10], Batch [400/469], Loss: 149.8765869140625
10:34:20 TRAINING Epoch [6/10], Batch [416/469], Loss: 150.41969108581543
10:34:20 TRAINING Epoch [6/10], Batch [432/469], Loss: 148.66820812225342
10:34:20 TRAINING Epoch [6/10], Batch [448/469], Loss: 148.67676448822021
10:34:20 TRAINING Epoch [6/10], Batch [464/469], Loss: 150.78193950653076
10:34:20 BEGIN VALIDATION Epoch [6/10]
10:34:21 VALIDATION Epoch [6/10], Loss: 150.8718586209454
10:34:21 BEGIN TRAINING Epoch [7/10]
10:34:21 TRAINING Epoch [7/10], Batch [16/469], Loss: 151.17117309570312
10:34:21 TRAINING Epoch [7/10], Batch [32/469], Loss: 148.71192455291748
10:34:21 TRAINING Epoch [7/10], Batch [48/469], Loss: 150.16570377349854
10:34:21 TRAINING Epoch [7/10], Batch [64/469], Loss: 150.73351573944092
10:34:21 TRAINING Epoch [7/10], Batch [80/469], Loss: 149.1097812652588
10:34:21 TRAINING Epoch [7/10], Batch [96/469], Loss: 149.17992496490479
10:34:21 TRAINING Epoch [7/10], Batch [112/469], Loss: 150.3316822052002
10:34:21 TRAINING Epoch [7/10], Batch [128/469], Loss: 150.34567070007324
10:34:21 TRAINING Epoch [7/10], Batch [144/469], Loss: 150.85293197631836
10:34:21 TRAINING Epoch [7/10], Batch [160/469], Loss: 151.469633102417
10:34:22 TRAINING Epoch [7/10], Batch [176/469], Loss: 149.4256763458252
10:34:22 TRAINING Epoch [7/10], Batch [192/469], Loss: 148.72124481201172
10:34:22 TRAINING Epoch [7/10], Batch [208/469], Loss: 150.76341438293457
10:34:22 TRAINING Epoch [7/10], Batch [224/469], Loss: 150.8609504699707
10:34:22 TRAINING Epoch [7/10], Batch [240/469], Loss: 150.26765155792236
10:34:22 TRAINING Epoch [7/10], Batch [256/469], Loss: 148.3953037261963
10:34:22 TRAINING Epoch [7/10], Batch [272/469], Loss: 148.9127254486084
10:34:22 TRAINING Epoch [7/10], Batch [288/469], Loss: 150.47169589996338
10:34:22 TRAINING Epoch [7/10], Batch [304/469], Loss: 150.87401676177979
10:34:22 TRAINING Epoch [7/10], Batch [320/469], Loss: 150.4116497039795
10:34:22 TRAINING Epoch [7/10], Batch [336/469], Loss: 150.6819133758545
10:34:22 TRAINING Epoch [7/10], Batch [352/469], Loss: 150.3094301223755
10:34:22 TRAINING Epoch [7/10], Batch [368/469], Loss: 149.22309684753418
10:34:22 TRAINING Epoch [7/10], Batch [384/469], Loss: 150.87175464630127
10:34:22 TRAINING Epoch [7/10], Batch [400/469], Loss: 149.2953987121582
10:34:23 TRAINING Epoch [7/10], Batch [416/469], Loss: 150.60057640075684
10:34:23 TRAINING Epoch [7/10], Batch [432/469], Loss: 150.39282321929932
10:34:23 TRAINING Epoch [7/10], Batch [448/469], Loss: 149.57011318206787
10:34:23 TRAINING Epoch [7/10], Batch [464/469], Loss: 149.72161674499512
10:34:23 BEGIN VALIDATION Epoch [7/10]
10:34:23 VALIDATION Epoch [7/10], Loss: 149.93698371211184
10:34:23 BEGIN TRAINING Epoch [8/10]
10:34:23 TRAINING Epoch [8/10], Batch [16/469], Loss: 148.36492729187012
10:34:23 TRAINING Epoch [8/10], Batch [32/469], Loss: 149.88623905181885
10:34:23 TRAINING Epoch [8/10], Batch [48/469], Loss: 148.89105606079102
10:34:24 TRAINING Epoch [8/10], Batch [64/469], Loss: 149.95507717132568
10:34:24 TRAINING Epoch [8/10], Batch [80/469], Loss: 147.53772354125977
10:34:24 TRAINING Epoch [8/10], Batch [96/469], Loss: 149.4766321182251
10:34:24 TRAINING Epoch [8/10], Batch [112/469], Loss: 149.90907287597656
10:34:24 TRAINING Epoch [8/10], Batch [128/469], Loss: 149.65767669677734
10:34:24 TRAINING Epoch [8/10], Batch [144/469], Loss: 150.33910369873047
10:34:24 TRAINING Epoch [8/10], Batch [160/469], Loss: 149.6848545074463
10:34:24 TRAINING Epoch [8/10], Batch [176/469], Loss: 147.3380069732666
10:34:24 TRAINING Epoch [8/10], Batch [192/469], Loss: 148.6942481994629
10:34:24 TRAINING Epoch [8/10], Batch [208/469], Loss: 148.02802276611328
10:34:24 TRAINING Epoch [8/10], Batch [224/469], Loss: 148.92097568511963
10:34:24 TRAINING Epoch [8/10], Batch [240/469], Loss: 149.24167728424072
10:34:24 TRAINING Epoch [8/10], Batch [256/469], Loss: 147.5538682937622
10:34:24 TRAINING Epoch [8/10], Batch [272/469], Loss: 149.17774486541748
10:34:24 TRAINING Epoch [8/10], Batch [288/469], Loss: 150.35798168182373
10:34:25 TRAINING Epoch [8/10], Batch [304/469], Loss: 150.60792064666748
10:34:25 TRAINING Epoch [8/10], Batch [320/469], Loss: 149.02231788635254
10:34:25 TRAINING Epoch [8/10], Batch [336/469], Loss: 149.0685224533081
10:34:25 TRAINING Epoch [8/10], Batch [352/469], Loss: 150.67023086547852
10:34:25 TRAINING Epoch [8/10], Batch [368/469], Loss: 146.9668960571289
10:34:25 TRAINING Epoch [8/10], Batch [384/469], Loss: 149.68322086334229
10:34:25 TRAINING Epoch [8/10], Batch [400/469], Loss: 150.04856491088867
10:34:25 TRAINING Epoch [8/10], Batch [416/469], Loss: 150.24073886871338
10:34:25 TRAINING Epoch [8/10], Batch [432/469], Loss: 149.05987358093262
10:34:25 TRAINING Epoch [8/10], Batch [448/469], Loss: 149.4359588623047
10:34:25 TRAINING Epoch [8/10], Batch [464/469], Loss: 149.5966510772705
10:34:25 BEGIN VALIDATION Epoch [8/10]
10:34:26 VALIDATION Epoch [8/10], Loss: 148.88787165774573
10:34:26 BEGIN TRAINING Epoch [9/10]
10:34:26 TRAINING Epoch [9/10], Batch [16/469], Loss: 147.40576267242432
10:34:26 TRAINING Epoch [9/10], Batch [32/469], Loss: 147.16540813446045
10:34:26 TRAINING Epoch [9/10], Batch [48/469], Loss: 148.47563648223877
10:34:26 TRAINING Epoch [9/10], Batch [64/469], Loss: 148.11021614074707
10:34:26 TRAINING Epoch [9/10], Batch [80/469], Loss: 148.67151546478271
10:34:26 TRAINING Epoch [9/10], Batch [96/469], Loss: 149.2308702468872
10:34:26 TRAINING Epoch [9/10], Batch [112/469], Loss: 149.60436248779297
10:34:26 TRAINING Epoch [9/10], Batch [128/469], Loss: 149.49626445770264
10:34:26 TRAINING Epoch [9/10], Batch [144/469], Loss: 147.06721687316895
10:34:26 TRAINING Epoch [9/10], Batch [160/469], Loss: 148.75034713745117
10:34:26 TRAINING Epoch [9/10], Batch [176/469], Loss: 149.14110946655273
10:34:26 TRAINING Epoch [9/10], Batch [192/469], Loss: 147.95206546783447
10:34:27 TRAINING Epoch [9/10], Batch [208/469], Loss: 150.53380870819092
10:34:27 TRAINING Epoch [9/10], Batch [224/469], Loss: 148.45255184173584
10:34:27 TRAINING Epoch [9/10], Batch [240/469], Loss: 148.16604042053223
10:34:27 TRAINING Epoch [9/10], Batch [256/469], Loss: 149.14571475982666
10:34:27 TRAINING Epoch [9/10], Batch [272/469], Loss: 147.14936447143555
10:34:27 TRAINING Epoch [9/10], Batch [288/469], Loss: 148.7846393585205
10:34:27 TRAINING Epoch [9/10], Batch [304/469], Loss: 149.21459865570068
10:34:27 TRAINING Epoch [9/10], Batch [320/469], Loss: 147.27066135406494
10:34:27 TRAINING Epoch [9/10], Batch [336/469], Loss: 149.77067279815674
10:34:27 TRAINING Epoch [9/10], Batch [352/469], Loss: 146.57568264007568
10:34:27 TRAINING Epoch [9/10], Batch [368/469], Loss: 148.18690490722656
10:34:27 TRAINING Epoch [9/10], Batch [384/469], Loss: 149.14636898040771
10:34:27 TRAINING Epoch [9/10], Batch [400/469], Loss: 149.94627857208252
10:34:27 TRAINING Epoch [9/10], Batch [416/469], Loss: 147.15710544586182
10:34:27 TRAINING Epoch [9/10], Batch [432/469], Loss: 147.78192615509033
10:34:28 TRAINING Epoch [9/10], Batch [448/469], Loss: 148.5593385696411
10:34:28 TRAINING Epoch [9/10], Batch [464/469], Loss: 147.16904830932617
10:34:28 BEGIN VALIDATION Epoch [9/10]
10:34:28 VALIDATION Epoch [9/10], Loss: 148.81379873541337
10:34:28 BEGIN TRAINING Epoch [10/10]
10:34:28 TRAINING Epoch [10/10], Batch [16/469], Loss: 146.22847366333008
10:34:28 TRAINING Epoch [10/10], Batch [32/469], Loss: 147.64885330200195
10:34:28 TRAINING Epoch [10/10], Batch [48/469], Loss: 147.55564212799072
10:34:28 TRAINING Epoch [10/10], Batch [64/469], Loss: 147.09041023254395
10:34:28 TRAINING Epoch [10/10], Batch [80/469], Loss: 149.1501111984253
10:34:28 TRAINING Epoch [10/10], Batch [96/469], Loss: 148.27759170532227
10:34:29 TRAINING Epoch [10/10], Batch [112/469], Loss: 148.09787940979004
10:34:29 TRAINING Epoch [10/10], Batch [128/469], Loss: 146.56356143951416
10:34:29 TRAINING Epoch [10/10], Batch [144/469], Loss: 149.64880561828613
10:34:29 TRAINING Epoch [10/10], Batch [160/469], Loss: 147.9294948577881
10:34:29 TRAINING Epoch [10/10], Batch [176/469], Loss: 148.68588066101074
10:34:29 TRAINING Epoch [10/10], Batch [192/469], Loss: 147.26053142547607
10:34:29 TRAINING Epoch [10/10], Batch [208/469], Loss: 147.07900047302246
10:34:29 TRAINING Epoch [10/10], Batch [224/469], Loss: 148.75758743286133
10:34:29 TRAINING Epoch [10/10], Batch [240/469], Loss: 148.41380882263184
10:34:29 TRAINING Epoch [10/10], Batch [256/469], Loss: 145.79871940612793
10:34:29 TRAINING Epoch [10/10], Batch [272/469], Loss: 146.22815895080566
10:34:29 TRAINING Epoch [10/10], Batch [288/469], Loss: 149.696195602417
10:34:29 TRAINING Epoch [10/10], Batch [304/469], Loss: 148.72713088989258
10:34:29 TRAINING Epoch [10/10], Batch [320/469], Loss: 150.67377185821533
10:34:29 TRAINING Epoch [10/10], Batch [336/469], Loss: 147.6783332824707
10:34:30 TRAINING Epoch [10/10], Batch [352/469], Loss: 147.76599025726318
10:34:30 TRAINING Epoch [10/10], Batch [368/469], Loss: 146.6891679763794
10:34:30 TRAINING Epoch [10/10], Batch [384/469], Loss: 147.3816614151001
10:34:30 TRAINING Epoch [10/10], Batch [400/469], Loss: 145.50618839263916
10:34:30 TRAINING Epoch [10/10], Batch [416/469], Loss: 148.2819004058838
10:34:30 TRAINING Epoch [10/10], Batch [432/469], Loss: 148.6860761642456
10:34:30 TRAINING Epoch [10/10], Batch [448/469], Loss: 146.50358486175537
10:34:30 TRAINING Epoch [10/10], Batch [464/469], Loss: 148.70681381225586
10:34:30 BEGIN VALIDATION Epoch [10/10]
10:34:30 VALIDATION Epoch [10/10], Loss: 148.22473859183395
10:34:30 Training complete.
